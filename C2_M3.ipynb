{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модуль 3 Разработка решения для определения показаний на аналоговом приборе. Создание пайплайна обучения и обучение модели. Проверка качества работы модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Построение пайплайна для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_directories(path):\n",
    "    # Получаем список всех элементов в указанной директории\n",
    "    items = os.listdir(path)\n",
    "    # Фильтруем только директории\n",
    "    directories = [item for item in items if os.path.isdir(os.path.join(path, item))]\n",
    "\n",
    "    return directories\n",
    "\n",
    "def list_files_in_directory(path):\n",
    "    # Получаем список всех элементов в указанной директории\n",
    "    items = os.listdir(path)\n",
    "    # Фильтруем только файлы\n",
    "    files = [os.path.join(path, item) for item in items if os.path.isfile(os.path.join(path, item))]\n",
    "\n",
    "    return files\n",
    "\n",
    "classes = list_directories(\"../images/Gauge_big/train/\")\n",
    "train_files = list_files_in_directory(\"../prepocessed_images/Gauge_big/train/\")\n",
    "test_files = list_files_in_directory(\"../prepocessed_images/Gauge_big/test/\")\n",
    "val_files = list_files_in_directory(\"../prepocessed_images/Gauge_big/val/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import tensorflow  as tf\n",
    "from tensorflow.python.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class ModelTrainingPipeline:\n",
    "    def __init__(self, model_type: str, **hyperparameters):\n",
    "        \"\"\"\n",
    "        model_type - тип модели на выбор\n",
    "        **hyperparameters - гиперпараметры для конкретной модели\n",
    "        \"\"\"\n",
    "        self.model_types: dict = { # Мои выбранные 3 типа моделей \n",
    "            \"CNN\": hyperparameters.get(\"CNN\", {}),\n",
    "            \"YOLO_CLASSIFY\": hyperparameters.get(\"YOLO_CLASSIFY\", {}),\n",
    "            \"YOLO_DETECT\": hyperparameters.get(\"YOLO_DETECT\", {}),\n",
    "        }\n",
    "\n",
    "        if model_type not in self.model_types:\n",
    "            raise ValueError(\"Выбран неверный тип модели\")\n",
    "\n",
    "        self.model = None\n",
    "        self.model_type = model_type\n",
    "        self.hyperparameters = self.model_types[model_type]\n",
    "\n",
    "        print(self.hyperparameters)\n",
    "\n",
    "        return self.evaluate_model_by_type()\n",
    "\n",
    "    def evaluate_model_by_type(self) -> list: # выполняет создание и обучение выбранной модели\n",
    "        \"\"\"\n",
    "        Возвращает параметры в следующем порядке:\n",
    "        1 - модель\n",
    "        2 - словарь с метриками\n",
    "        3 - время в секундах на эпоху\n",
    "        4 - суммарное время выполнения в минутах\n",
    "        \"\"\"\n",
    "        self.pipeline_start_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if self.model_type == \"CNN\":\n",
    "            self.create_cnn_model()\n",
    "            train_dataset = self.create_dataset(train_files, classes, 8)\n",
    "            test_dataset = self.create_dataset(test_files, classes, 8) # Преобразуем файлы в датасеты для CNN\n",
    "            val_dataset = self.create_dataset(val_files, classes, 8)\n",
    "            self.train_cnn_model(train_dataset, test_dataset, val_dataset)\n",
    "        \n",
    "        elif self.model_type == \"YOLO_CLASSIFY\":\n",
    "            self.create_yolo_classify_model()\n",
    "            self.evaluate_yolo_training()\n",
    "\n",
    "        elif self.model_type == \"YOLO_DETECT\":\n",
    "            self.create_yolo_decect_model()\n",
    "            self.evaluate_yolo_training()\n",
    "            \n",
    "        self.time_per_epoch = (self.total_time * 60) / self.hyperparameters[\"epochs\"]\n",
    "        results = [self.model, {\"accuracy\": self.accuracy, \"roc_auc\": self.roc_auc, \"f1\": self.f1}, self.time_per_epoch, self.total_time]\n",
    "        self.save_results_to_file(results)\n",
    "        return results\n",
    "\n",
    "    def save_results_to_file(self, results):\n",
    "        filename = f\"training_results_{self.model_type}_{self.pipeline_start_time}.txt\"\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"Результаты обучения модели:\\n\")\n",
    "            for key, value in results.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "            f.write(f\"Общее время обучения (мин): {self.total_time}\\n\")\n",
    "        print(f\"Результаты сохранены в файл: {filename}\")\n",
    "\n",
    "    def create_yolo_decect_model(self):\n",
    "        # Создает модель YOLO для детекции\n",
    "        self.model = YOLO(\"../models/yolov8s.pt\", verbose=True)\n",
    "        if not self.model:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def create_yolo_classify_model(self):\n",
    "        # Создает модель YOLO для детекции\n",
    "        self.model = YOLO(\"../models/yolov8s-cls.pt\", verbose=True)\n",
    "        if not self.model:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def evaluate_yolo_training(self): # YOLO проводит валидацию после каждой эпохи обучения или заданного количества итераций, чтобы оценить производительность модели на данных валидации\n",
    "        # Запускает обучение любой из моделей YOLO\n",
    "        start_time = time.time()\n",
    "        self.results = self.model.train(\n",
    "            data=\"data.yaml\",\n",
    "            imgsz=244,\n",
    "            epochs=self.hyperparameters['epochs'],\n",
    "            batch=self.hyperparameters['batch'],\n",
    "            lr0=self.hyperparameters['lr0'],\n",
    "            momentum=self.hyperparameters['momentum'],\n",
    "            weight_decay=self.hyperparameters['weight_decay'],\n",
    "            name=\"yolo8vn_detection\",\n",
    "        )\n",
    "        self.total_time = (time.time() - start_time) / 60\n",
    "\n",
    "        self.yolo_final_validation()\n",
    "\n",
    "    def yolo_final_validation(self):\n",
    "        val_results = self.model.val(data=\"data.yaml\")\n",
    "        print(\"Итоговые результаты валидации:\")\n",
    "        print(val_results)\n",
    "\n",
    "\n",
    "        val_images, val_labels = val_files, classes\n",
    "        predictions = self.model.predict(val_images)\n",
    "\n",
    "        # Обработка предсказаний\n",
    "        predicted_classes = np.array([np.argmax(pred) for pred in predictions])\n",
    "\n",
    "        # Расчет метрик\n",
    "        self.accuracy = accuracy_score(val_labels, predicted_classes)\n",
    "        self.roc_auc = roc_auc_score(val_labels, predictions, multi_class='ovr')\n",
    "        self.f1 = f1_score(val_labels, predicted_classes, average='macro')\n",
    "\n",
    "        print(f\"Итоговая точность: {self.accuracy}\")\n",
    "        print(f\"ROC AUC Score: {self.roc_auc}\")\n",
    "        print(f\"F1 Score (macro): {self.f1}\")\n",
    "\n",
    "\n",
    "    def create_cnn_model(self):\n",
    "        # Создание простой CNN модели\n",
    "\n",
    "        self.model = models.Sequential()\n",
    "        self.model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(self.hyperparameters['img_size'], self.hyperparameters['img_size'], 3)))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        self.model.add(layers.MaxPooling2D((2, 2)))\n",
    "        self.model.add(layers.Flatten())\n",
    "        self.model.add(layers.Dense(128, activation='relu'))\n",
    "        self.model.add(layers.Dense(self.hyperparameters['num_classes'], activation='softmax'))  # Для многоклассовой классификации\n",
    "        # Компиляция модели\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.hyperparameters['lr']),\n",
    "                           loss='sparse_categorical_crossentropy',  # Или 'categorical_crossentropy' в зависимости от формата меток\n",
    "                           metrics=['accuracy'])\n",
    "        \n",
    "    def load_images_tf(self, image_path):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = img / 255.0 # нормализация\n",
    "        return img\n",
    "    \n",
    "    def create_dataset(self, image_paths, labels, batch_size):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(lambda x, y: (self.load_images_tf(x), y))\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "    def train_cnn_model(self, train_data, test_data, val_data):\n",
    "        # Метод для обучения CNN модели\n",
    "        start_time = time.time()\n",
    "        history = self.model.fit(train_data, validation_data=test_data, epochs=self.hyperparameters['epochs'], batch_size=self.hyperparameters['batch'])\n",
    "        \n",
    "        # Промежуточное тестирование CNN модели\n",
    "        for epoch in range(self.hyperparameters['epochs']):\n",
    "            print(f\"Epoch {epoch + 1}/{self.hyperparameters['epochs']}\")\n",
    "            self.model.fit(train_data, validation_data=test_data, epochs=1, batch_size=self.hyperparameters['batch'])\n",
    "            test_loss, test_acc = self.model.evaluate(test_data)\n",
    "            print(f\"Test Loss: {test_loss}, Test Accuracy: {test_acc}\")\n",
    "\n",
    "        self.total_time = (time.time() - start_time) / 60\n",
    "\n",
    "        test_loss, test_acc = self.model.evaluate(val_data)\n",
    "        print(f\"Итоговая потеря на валидационной выборке: {test_loss}, Итоговая точность: {test_acc}\")\n",
    "\n",
    "        #Получение предсказаний для расчета метрик\n",
    "        val_images, val_labels = next(iter(val_data))\n",
    "        predictions = self.model.predict(val_images)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Расчет метрик\n",
    "        self.accuracy = accuracy_score(val_labels, predicted_classes)\n",
    "        self.roc_auc = roc_auc_score(val_labels, predictions, multi_class='ovr')\n",
    "        self.f1 = f1_score(val_labels, predicted_classes, average='macro')\n",
    "\n",
    "        print(f\"Итоговая точность: {self.accuracy}\")\n",
    "        print(f\"ROC AUC Score: {self.roc_auc}\")\n",
    "        print(f\"F1 Score (macro): {self.f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    CNN: Эффективны для обработки изображений, способны извлекать пространственные признаки, что важно для точного считывания показаний с циферблата\n",
    "\n",
    "    YOLO: Обеспечивает высокую скорость и точность в детекции объектов, что полезно для быстрого определения показаний на аналоговых приборах в реальном времени. В рамках разных подходов к задаче были выбраны разные модели классификации и детекции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример запуска модели YOLO_DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ModelTrainingPipeline(\"YOLO_DETECT\", YOLO_DECECT={\"epochs\": 1, \"batch\": 8, \"lr0\": 0.5, \"momentum\": 0.7, \"weight_decay\": 0.2})\n",
    "# Ячейка не запущена т.к. ГЭ сказал, что сразу можно не обучать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример запуска модели YOLO_CLASSIFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = ModelTrainingPipeline(\"YOLO_CLASSIFY\", YOLO_CLASSIFY={\"epochs\": 5, \"batch\": 8, \"lr0\": 0.5, \"momentum\": 0.7, \"weight_decay\": 0.2})\n",
    "# Ячейка не запущена т.к. ГЭ сказал, что сразу можно не обучать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример запуска модели CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = ModelTrainingPipeline(\"CNN\", CNN={\"img_size\": 244, \"epochs\": 5 })\n",
    "# Ячейка не запущена т.к. ГЭ сказал, что сразу можно не обучать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подключение вечернего обучения с Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to load the config, contains a configuration error.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to configure formatter 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:93\u001b[0m, in \u001b[0;36m_resolve\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(found, n)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'airflow' has no attribute 'utils' (most likely due to a circular import)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:541\u001b[0m, in \u001b[0;36mDictConfigurator.configure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m     formatters[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_formatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mformatters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:673\u001b[0m, in \u001b[0;36mDictConfigurator.configure_formatter\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# A TypeError would be raised if \"validate\" key is passed in with a formatter callable\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;66;03m# that does not accept \"validate\" as a parameter\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:96\u001b[0m, in \u001b[0;36m_resolve\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28m__import__\u001b[39m(used)\n\u001b[0;32m---> 96\u001b[0m         found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(found, n)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m found\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'airflow' has no attribute 'utils' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PythonOperator\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/airflow/__init__.py:74\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Perform side-effects unless someone has explicitly opted out before import\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# WARNING: DO NOT USE THIS UNLESS YOU REALLY KNOW WHAT YOU'RE DOING.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# This environment variable prevents proper initialization, and things like\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# configs, logging, the ORM, etc. will be broken. It is only useful if you only\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# access certain trivial constants and free functions (e.g. `__version__`).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_AIRFLOW__AS_LIBRARY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 74\u001b[0m     \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Things to lazy import in form {local_name: ('target_module', 'target_name', 'deprecated')}\u001b[39;00m\n\u001b[1;32m     77\u001b[0m __lazy_imports: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDAG\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.models.dag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDAG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirflowException\u001b[39m\u001b[38;5;124m\"\u001b[39m: (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.exceptions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAirflowException\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     84\u001b[0m }\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/airflow/settings.py:785\u001b[0m, in \u001b[0;36minitialize\u001b[0;34m()\u001b[0m\n\u001b[1;32m    783\u001b[0m prepare_syspath_for_dags_folder()\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m LOGGING_CLASS_PATH\n\u001b[0;32m--> 785\u001b[0m LOGGING_CLASS_PATH \u001b[38;5;241m=\u001b[39m \u001b[43mconfigure_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    786\u001b[0m State\u001b[38;5;241m.\u001b[39mstate_color\u001b[38;5;241m.\u001b[39mupdate(STATE_COLORS)\n\u001b[1;32m    788\u001b[0m configure_adapters()\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/airflow/logging_config.py:74\u001b[0m, in \u001b[0;36mconfigure_logging\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load the config, contains a configuration error.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# When there is an error in the config, escalate the exception\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# otherwise Airflow would silently fall back on the default config\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     76\u001b[0m validate_logging_config(logging_config)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logging_class_path\n",
      "File \u001b[0;32m~/main_venv/lib/python3.11/site-packages/airflow/logging_config.py:69\u001b[0m, in \u001b[0;36mconfigure_logging\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             task_handler_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_secrets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Try to init logging\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[43mdictConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogging_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mKeyError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     71\u001b[0m     log\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load the config, contains a configuration error.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:812\u001b[0m, in \u001b[0;36mdictConfig\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdictConfig\u001b[39m(config):\n\u001b[1;32m    811\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Configure logging using a dictionary.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     \u001b[43mdictConfigClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/logging/config.py:544\u001b[0m, in \u001b[0;36mDictConfigurator.configure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         formatters[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_formatter(\n\u001b[1;32m    542\u001b[0m                                             formatters[name])\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to configure \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    545\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mformatter \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m name) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Next, do filters - they don't refer to anything else, either\u001b[39;00m\n\u001b[1;32m    547\u001b[0m filters \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m, EMPTY_DICT)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to configure formatter 'airflow'"
     ]
    }
   ],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "# import logging\n",
    "\n",
    "from airflow.logging_config import DEFAULT_LOGGING_CONFIG\n",
    "\n",
    "\n",
    "LOGGING_CONFIG = DEFAULT_LOGGING_CONFIG\n",
    "\n",
    "LOGGING_CONFIG[\"formatters\"][\"airflow\"] = {\n",
    "\n",
    "    \"format\": \"[%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s\"\n",
    "\n",
    "}\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=30),\n",
    "    'execution_timeout': timedelta(hours=13, minutes=30),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nightly_model_training',\n",
    "    default_args=default_args,\n",
    "    description='Ночное обучение моделей компьютерного зрения',\n",
    "    schedule_interval='0 18 * * *',  # 18:00 ежедневно\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    ")\n",
    "\n",
    "def get_hyperparameters(model_type):\n",
    "    configs = {\n",
    "        \"CNN\": {\n",
    "            \"img_size\": 224,\n",
    "            \"num_classes\": 1000,\n",
    "            \"epochs\": 20,\n",
    "            \"batch\": 64,\n",
    "            \"lr\": 0.0001\n",
    "        },\n",
    "        \"YOLO_CLASSIFY\": {\n",
    "            \"epochs\": 50,\n",
    "            \"batch\": 32,\n",
    "            \"lr0\": 0.01,\n",
    "            \"momentum\": 0.937,\n",
    "            \"weight_decay\": 0.0005\n",
    "        },\n",
    "        \"YOLO_DETECT\": {\n",
    "            \"epochs\": 100,\n",
    "            \"batch\": 16,\n",
    "            \"lr0\": 0.01,\n",
    "            \"momentum\": 0.9,\n",
    "            \"weight_decay\": 0.0001\n",
    "        }\n",
    "    }\n",
    "    return configs.get(model_type, {})\n",
    "\n",
    "def load_training_data():\n",
    "    # Реализуйте загрузку и предобработку данных\n",
    "    return {\n",
    "        'train_files': [],\n",
    "        'test_files': [],\n",
    "        'val_files': [],\n",
    "        'classes': []\n",
    "    }\n",
    "\n",
    "def train_model_wrapper(**kwargs):\n",
    "    try:\n",
    "        model_type = \"CNN\"  # Можно параметризировать\n",
    "        \n",
    "        # Загрузка данных\n",
    "        data = load_training_data()\n",
    "        \n",
    "        # Получение гиперпараметров\n",
    "        hyperparams = get_hyperparameters(model_type)\n",
    "        \n",
    "        # Инициализация и запуск пайплайна\n",
    "        pipeline = ModelTrainingPipeline(\n",
    "            model_type=model_type,\n",
    "            train_files=data['train_files'],\n",
    "            test_files=data['test_files'],\n",
    "            val_files=data['val_files'],\n",
    "            classes=data['classes'],\n",
    "            **hyperparams\n",
    "        )\n",
    "        \n",
    "        # logging.info(\"Обучение успешно завершено\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        # logging.error(f\"Ошибка обучения: {str(e)}\")\n",
    "        # Дополнительная обработка ошибок\n",
    "        error_report = f\"Error report:\\n{str(e)}\"\n",
    "        with open(f\"/reports/error_{datetime.now().isoformat()}.txt\", \"w\") as f:\n",
    "            f.write(error_report)\n",
    "        return True  # Возвращаем успех чтобы не блокировать последующие выполнения\n",
    "\n",
    "training_task = PythonOperator(\n",
    "    task_id='model_training_task',\n",
    "    python_callable=train_model_wrapper,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "training_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
